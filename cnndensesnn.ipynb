{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0,), (1,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(\n",
    "    mnist_train, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    mnist_test, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "\n",
    "\n",
    "class CNNDenseSNN(nn.Module):\n",
    "    def __init__(self, beta=0.95):\n",
    "        super(CNNDenseSNN, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 9 * 9)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x0, filters=None):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "\n",
    "        for _ in range(10):\n",
    "            if filters is not None:\n",
    "                outputs = []\n",
    "                for i in range(x0.shape[0]):\n",
    "                    output = F.conv2d(\n",
    "                        x0[i].unsqueeze(0),\n",
    "                        filters[i].unsqueeze(0).unsqueeze(0),\n",
    "                        padding=1,\n",
    "                    )\n",
    "                    outputs.append(output.reshape(1, 28, 28))\n",
    "\n",
    "                x0 = x0 + F.relu(torch.stack(outputs, dim=0)) / 10\n",
    "\n",
    "            x = self.pool(F.relu(self.conv1(x0)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "            x = x.flatten(1)\n",
    "\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            filter_x = F.relu(self.fc3(x))\n",
    "\n",
    "            spk1, mem1 = self.lif1(filter_x, mem1)\n",
    "            filters = spk1.view(-1, 3, 3)\n",
    "\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = CNNDenseSNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 2.2970423698425293\n",
      "Iteration: 1 \t Train Loss: 2.299041509628296\n",
      "Iteration: 2 \t Train Loss: 2.290811538696289\n",
      "Iteration: 3 \t Train Loss: 2.2823262214660645\n",
      "Iteration: 4 \t Train Loss: 2.2812108993530273\n",
      "Iteration: 5 \t Train Loss: 2.256357192993164\n",
      "Iteration: 6 \t Train Loss: 2.2369134426116943\n",
      "Iteration: 7 \t Train Loss: 2.2182374000549316\n",
      "Iteration: 8 \t Train Loss: 2.170041561126709\n",
      "Iteration: 9 \t Train Loss: 2.1805713176727295\n",
      "Iteration: 10 \t Train Loss: 2.100017547607422\n",
      "Iteration: 11 \t Train Loss: 2.0457100868225098\n",
      "Iteration: 12 \t Train Loss: 1.9376020431518555\n",
      "Iteration: 13 \t Train Loss: 2.0795230865478516\n",
      "Iteration: 14 \t Train Loss: 1.959992527961731\n",
      "Iteration: 15 \t Train Loss: 1.7186599969863892\n",
      "Iteration: 16 \t Train Loss: 1.6078206300735474\n",
      "Iteration: 17 \t Train Loss: 1.5651899576187134\n",
      "Iteration: 18 \t Train Loss: 1.2995526790618896\n",
      "Iteration: 19 \t Train Loss: 1.3713992834091187\n",
      "Iteration: 20 \t Train Loss: 1.226878046989441\n",
      "Iteration: 21 \t Train Loss: 0.988494336605072\n",
      "Iteration: 22 \t Train Loss: 1.178733468055725\n",
      "Iteration: 23 \t Train Loss: 1.0867249965667725\n",
      "Iteration: 24 \t Train Loss: 1.231452465057373\n",
      "Iteration: 25 \t Train Loss: 0.8016699552536011\n",
      "Iteration: 26 \t Train Loss: 1.4054206609725952\n",
      "Iteration: 27 \t Train Loss: 1.049186110496521\n",
      "Iteration: 28 \t Train Loss: 0.8676836490631104\n",
      "Iteration: 29 \t Train Loss: 0.8315443396568298\n",
      "Iteration: 30 \t Train Loss: 0.8925604224205017\n",
      "Iteration: 31 \t Train Loss: 0.9692153930664062\n",
      "Iteration: 32 \t Train Loss: 0.7984905242919922\n",
      "Iteration: 33 \t Train Loss: 0.8056468963623047\n",
      "Iteration: 34 \t Train Loss: 0.770160436630249\n",
      "Iteration: 35 \t Train Loss: 0.7246977686882019\n",
      "Iteration: 36 \t Train Loss: 0.7882645130157471\n",
      "Iteration: 37 \t Train Loss: 0.7200222611427307\n",
      "Iteration: 38 \t Train Loss: 0.6309252381324768\n",
      "Iteration: 39 \t Train Loss: 0.610138475894928\n",
      "Iteration: 40 \t Train Loss: 0.7974647283554077\n",
      "Iteration: 41 \t Train Loss: 0.6626254320144653\n",
      "Iteration: 42 \t Train Loss: 0.6713579893112183\n",
      "Iteration: 43 \t Train Loss: 0.7809489369392395\n",
      "Iteration: 44 \t Train Loss: 0.6747322678565979\n",
      "Iteration: 45 \t Train Loss: 0.5438072085380554\n",
      "Iteration: 46 \t Train Loss: 0.6205962300300598\n",
      "Iteration: 47 \t Train Loss: 0.6171523928642273\n",
      "Iteration: 48 \t Train Loss: 0.36517277359962463\n",
      "Iteration: 49 \t Train Loss: 0.5020567178726196\n",
      "Iteration: 50 \t Train Loss: 0.6653633713722229\n",
      "Iteration: 51 \t Train Loss: 0.524989664554596\n",
      "Iteration: 52 \t Train Loss: 0.545644998550415\n",
      "Iteration: 53 \t Train Loss: 0.429299920797348\n",
      "Iteration: 54 \t Train Loss: 0.4640178084373474\n",
      "Iteration: 55 \t Train Loss: 0.40418878197669983\n",
      "Iteration: 56 \t Train Loss: 0.39640551805496216\n",
      "Iteration: 57 \t Train Loss: 0.5373367071151733\n",
      "Iteration: 58 \t Train Loss: 0.3171304166316986\n",
      "Iteration: 59 \t Train Loss: 0.3520297110080719\n",
      "Iteration: 60 \t Train Loss: 0.5041881799697876\n",
      "Iteration: 61 \t Train Loss: 0.27565211057662964\n",
      "Iteration: 62 \t Train Loss: 0.4392716586589813\n",
      "Iteration: 63 \t Train Loss: 0.23950287699699402\n",
      "Iteration: 64 \t Train Loss: 0.4771038591861725\n",
      "Iteration: 65 \t Train Loss: 0.3623286783695221\n",
      "Iteration: 66 \t Train Loss: 0.6235546469688416\n",
      "Iteration: 67 \t Train Loss: 0.47258520126342773\n",
      "Iteration: 68 \t Train Loss: 0.47120392322540283\n",
      "Iteration: 69 \t Train Loss: 0.45576024055480957\n",
      "Iteration: 70 \t Train Loss: 0.2441558837890625\n",
      "Iteration: 71 \t Train Loss: 0.38986462354660034\n",
      "Iteration: 72 \t Train Loss: 0.5484938025474548\n",
      "Iteration: 73 \t Train Loss: 0.37519392371177673\n",
      "Iteration: 74 \t Train Loss: 0.4037538766860962\n",
      "Iteration: 75 \t Train Loss: 0.28036990761756897\n",
      "Iteration: 76 \t Train Loss: 0.4320853352546692\n",
      "Iteration: 77 \t Train Loss: 0.43565836548805237\n",
      "Iteration: 78 \t Train Loss: 0.4299050271511078\n",
      "Iteration: 79 \t Train Loss: 0.364837646484375\n",
      "Iteration: 80 \t Train Loss: 0.5650464296340942\n",
      "Iteration: 81 \t Train Loss: 0.41795384883880615\n",
      "Iteration: 82 \t Train Loss: 0.22155435383319855\n",
      "Iteration: 83 \t Train Loss: 0.3964419960975647\n",
      "Iteration: 84 \t Train Loss: 0.3316859304904938\n",
      "Iteration: 85 \t Train Loss: 0.45926520228385925\n",
      "Iteration: 86 \t Train Loss: 0.5435553789138794\n",
      "Iteration: 87 \t Train Loss: 0.336660772562027\n",
      "Iteration: 88 \t Train Loss: 0.34756672382354736\n",
      "Iteration: 89 \t Train Loss: 0.3441542387008667\n",
      "Iteration: 90 \t Train Loss: 0.5842722654342651\n",
      "Iteration: 91 \t Train Loss: 0.2663424611091614\n",
      "Iteration: 92 \t Train Loss: 0.26490363478660583\n",
      "Iteration: 93 \t Train Loss: 0.34775838255882263\n",
      "Iteration: 94 \t Train Loss: 0.3364894986152649\n",
      "Iteration: 95 \t Train Loss: 0.39781618118286133\n",
      "Iteration: 96 \t Train Loss: 0.249406635761261\n",
      "Iteration: 97 \t Train Loss: 0.262209415435791\n",
      "Iteration: 98 \t Train Loss: 0.4111866354942322\n",
      "Iteration: 99 \t Train Loss: 0.20937351882457733\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        result = net(data)\n",
    "\n",
    "        loss_val = torch.zeros((1), device=device)\n",
    "        loss_val = loss(result, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        # if counter % 10 == 0:\n",
    "        print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "\n",
    "\n",
    "class CNNDenseSNN(nn.Module):\n",
    "    def __init__(self, beta=0.9):\n",
    "        super(CNNDenseSNN, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 9 * 9)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x0, filters=None):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "\n",
    "        for _ in range(10):\n",
    "            if filters is not None:\n",
    "                outputs = []\n",
    "                for i in range(x0.shape[0]):\n",
    "                    output = F.conv2d(\n",
    "                        x0[i].unsqueeze(0),\n",
    "                        filters[i].unsqueeze(0).unsqueeze(0),\n",
    "                        padding=1,\n",
    "                    )\n",
    "                    outputs.append(output.reshape(1, 28, 28))\n",
    "\n",
    "                x0 = x0 + F.relu(torch.stack(outputs, dim=0)) / 8\n",
    "\n",
    "            x = self.pool(F.relu(self.conv1(x0)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "            x = x.flatten(1)\n",
    "\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            filter_x = F.relu(self.fc3(x))\n",
    "\n",
    "            spk1, mem1 = self.lif1(filter_x, mem1)\n",
    "            filters = spk1.view(-1, 3, 3)\n",
    "\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = CNNDenseSNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 2.3100595474243164\n",
      "Iteration: 1 \t Train Loss: 2.300555467605591\n",
      "Iteration: 2 \t Train Loss: 2.301348924636841\n",
      "Iteration: 3 \t Train Loss: 2.289250612258911\n",
      "Iteration: 4 \t Train Loss: 2.283111572265625\n",
      "Iteration: 5 \t Train Loss: 2.263258695602417\n",
      "Iteration: 6 \t Train Loss: 2.276064395904541\n",
      "Iteration: 7 \t Train Loss: 2.2461369037628174\n",
      "Iteration: 8 \t Train Loss: 2.2103350162506104\n",
      "Iteration: 9 \t Train Loss: 2.19960618019104\n",
      "Iteration: 10 \t Train Loss: 2.1158485412597656\n",
      "Iteration: 11 \t Train Loss: 2.1273183822631836\n",
      "Iteration: 12 \t Train Loss: 2.0381686687469482\n",
      "Iteration: 13 \t Train Loss: 1.772718906402588\n",
      "Iteration: 14 \t Train Loss: 1.8430920839309692\n",
      "Iteration: 15 \t Train Loss: 1.722664713859558\n",
      "Iteration: 16 \t Train Loss: 1.6047680377960205\n",
      "Iteration: 17 \t Train Loss: 1.488071084022522\n",
      "Iteration: 18 \t Train Loss: 1.4315389394760132\n",
      "Iteration: 19 \t Train Loss: 1.1387362480163574\n",
      "Iteration: 20 \t Train Loss: 1.005287766456604\n",
      "Iteration: 21 \t Train Loss: 1.32261061668396\n",
      "Iteration: 22 \t Train Loss: 0.975459098815918\n",
      "Iteration: 23 \t Train Loss: 1.2616559267044067\n",
      "Iteration: 24 \t Train Loss: 1.030079960823059\n",
      "Iteration: 25 \t Train Loss: 1.0382814407348633\n",
      "Iteration: 26 \t Train Loss: 0.977620542049408\n",
      "Iteration: 27 \t Train Loss: 1.168783187866211\n",
      "Iteration: 28 \t Train Loss: 0.8874664306640625\n",
      "Iteration: 29 \t Train Loss: 0.9404969811439514\n",
      "Iteration: 30 \t Train Loss: 0.873822033405304\n",
      "Iteration: 31 \t Train Loss: 0.9452989101409912\n",
      "Iteration: 32 \t Train Loss: 0.6303626894950867\n",
      "Iteration: 33 \t Train Loss: 0.7313615083694458\n",
      "Iteration: 34 \t Train Loss: 0.7987856268882751\n",
      "Iteration: 35 \t Train Loss: 0.7782388925552368\n",
      "Iteration: 36 \t Train Loss: 0.6921040415763855\n",
      "Iteration: 37 \t Train Loss: 0.9090147614479065\n",
      "Iteration: 38 \t Train Loss: 0.6385819911956787\n",
      "Iteration: 39 \t Train Loss: 0.6021387577056885\n",
      "Iteration: 40 \t Train Loss: 0.6572213768959045\n",
      "Iteration: 41 \t Train Loss: 0.7255609035491943\n",
      "Iteration: 42 \t Train Loss: 0.6520230770111084\n",
      "Iteration: 43 \t Train Loss: 0.5327650308609009\n",
      "Iteration: 44 \t Train Loss: 0.3762545883655548\n",
      "Iteration: 45 \t Train Loss: 0.5811018347740173\n",
      "Iteration: 46 \t Train Loss: 0.5924479365348816\n",
      "Iteration: 47 \t Train Loss: 0.6837987303733826\n",
      "Iteration: 48 \t Train Loss: 0.45931828022003174\n",
      "Iteration: 49 \t Train Loss: 0.4960553050041199\n",
      "Iteration: 50 \t Train Loss: 0.5093798041343689\n",
      "Iteration: 51 \t Train Loss: 0.5494148135185242\n",
      "Iteration: 52 \t Train Loss: 0.5131650567054749\n",
      "Iteration: 53 \t Train Loss: 0.4857720732688904\n",
      "Iteration: 54 \t Train Loss: 0.814663290977478\n",
      "Iteration: 55 \t Train Loss: 0.4358699321746826\n",
      "Iteration: 56 \t Train Loss: 0.6905120611190796\n",
      "Iteration: 57 \t Train Loss: 0.6010445356369019\n",
      "Iteration: 58 \t Train Loss: 0.4622291326522827\n",
      "Iteration: 59 \t Train Loss: 0.5046658515930176\n",
      "Iteration: 60 \t Train Loss: 0.5393716096878052\n",
      "Iteration: 61 \t Train Loss: 0.5025659203529358\n",
      "Iteration: 62 \t Train Loss: 0.5174944400787354\n",
      "Iteration: 63 \t Train Loss: 0.5359266996383667\n",
      "Iteration: 64 \t Train Loss: 0.5090553164482117\n",
      "Iteration: 65 \t Train Loss: 0.4443540871143341\n",
      "Iteration: 66 \t Train Loss: 0.3381112217903137\n",
      "Iteration: 67 \t Train Loss: 0.4362904727458954\n",
      "Iteration: 68 \t Train Loss: 0.5128833651542664\n",
      "Iteration: 69 \t Train Loss: 0.3257458209991455\n",
      "Iteration: 70 \t Train Loss: 0.4732517600059509\n",
      "Iteration: 71 \t Train Loss: 0.4589133560657501\n",
      "Iteration: 72 \t Train Loss: 0.28168490529060364\n",
      "Iteration: 73 \t Train Loss: 0.34625014662742615\n",
      "Iteration: 74 \t Train Loss: 0.2716033458709717\n",
      "Iteration: 75 \t Train Loss: 0.2704930901527405\n",
      "Iteration: 76 \t Train Loss: 0.54129958152771\n",
      "Iteration: 77 \t Train Loss: 0.25997498631477356\n",
      "Iteration: 78 \t Train Loss: 0.43140116333961487\n",
      "Iteration: 79 \t Train Loss: 0.2788594365119934\n",
      "Iteration: 80 \t Train Loss: 0.5779986381530762\n",
      "Iteration: 81 \t Train Loss: 0.16557937860488892\n",
      "Iteration: 82 \t Train Loss: 0.25455573201179504\n",
      "Iteration: 83 \t Train Loss: 0.3565572500228882\n",
      "Iteration: 84 \t Train Loss: 0.32393765449523926\n",
      "Iteration: 85 \t Train Loss: 0.34004539251327515\n",
      "Iteration: 86 \t Train Loss: 0.3495464324951172\n",
      "Iteration: 87 \t Train Loss: 0.2897624671459198\n",
      "Iteration: 88 \t Train Loss: 0.2804514765739441\n",
      "Iteration: 89 \t Train Loss: 0.3708760738372803\n",
      "Iteration: 90 \t Train Loss: 0.27096039056777954\n",
      "Iteration: 91 \t Train Loss: 0.4082169830799103\n",
      "Iteration: 92 \t Train Loss: 0.3413381576538086\n",
      "Iteration: 93 \t Train Loss: 0.24633203446865082\n",
      "Iteration: 94 \t Train Loss: 0.24387064576148987\n",
      "Iteration: 95 \t Train Loss: 0.354856014251709\n",
      "Iteration: 96 \t Train Loss: 0.26945531368255615\n",
      "Iteration: 97 \t Train Loss: 0.40300285816192627\n",
      "Iteration: 98 \t Train Loss: 0.2653665244579315\n",
      "Iteration: 99 \t Train Loss: 0.24196037650108337\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        result = net(data)\n",
    "\n",
    "        loss_val = torch.zeros((1), device=device)\n",
    "        loss_val = loss(result, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        # if counter % 10 == 0:\n",
    "        print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Test Accuracy: 90.85%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        print(f\"Iteration: {counter}\")\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Test Accuracy: 93.03%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        print(f\"Iteration: {counter}\")\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "\n",
    "\n",
    "class CNNDenseSNN(nn.Module):\n",
    "    def __init__(self, beta=0.9):\n",
    "        super(CNNDenseSNN, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 9 * 9)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x0, filters=None):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "\n",
    "        for j in range(10):\n",
    "            if filters is not None:\n",
    "                outputs = []\n",
    "                for i in range(x0.shape[0]):\n",
    "                    output = F.conv2d(\n",
    "                        x0[i].unsqueeze(0),\n",
    "                        filters[i].unsqueeze(0).unsqueeze(0),\n",
    "                        padding=1,\n",
    "                    )\n",
    "                    outputs.append(output.reshape(1, 28, 28))\n",
    "\n",
    "                # x0 = x0 + F.relu(torch.stack(outputs, dim=0)) / 8\n",
    "\n",
    "                x0 = (\n",
    "                    x0\n",
    "                    + pow(-1, j)\n",
    "                    * torch.exp(torch.tensor((-j / 16)))\n",
    "                    * torch.stack(outputs, dim=0)\n",
    "                    / 5\n",
    "                )\n",
    "\n",
    "            x = self.pool(F.relu(self.conv1(x0)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "            x = x.flatten(1)\n",
    "\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            filter_x = F.relu(self.fc3(x))\n",
    "\n",
    "            spk1, mem1 = self.lif1(filter_x, mem1)\n",
    "            filters = spk1.view(-1, 3, 3)\n",
    "\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = CNNDenseSNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 2.3011057376861572\n",
      "Iteration: 1 \t Train Loss: 2.298447608947754\n",
      "Iteration: 2 \t Train Loss: 2.2773585319519043\n",
      "Iteration: 3 \t Train Loss: 2.273264169692993\n",
      "Iteration: 4 \t Train Loss: 2.254110813140869\n",
      "Iteration: 5 \t Train Loss: 2.2486047744750977\n",
      "Iteration: 6 \t Train Loss: 2.230839729309082\n",
      "Iteration: 7 \t Train Loss: 2.218226671218872\n",
      "Iteration: 8 \t Train Loss: 2.1741645336151123\n",
      "Iteration: 9 \t Train Loss: 2.1841020584106445\n",
      "Iteration: 10 \t Train Loss: 2.118741750717163\n",
      "Iteration: 11 \t Train Loss: 2.081988573074341\n",
      "Iteration: 12 \t Train Loss: 2.089444160461426\n",
      "Iteration: 13 \t Train Loss: 2.04512882232666\n",
      "Iteration: 14 \t Train Loss: 2.011805295944214\n",
      "Iteration: 15 \t Train Loss: 1.9291667938232422\n",
      "Iteration: 16 \t Train Loss: 1.9229484796524048\n",
      "Iteration: 17 \t Train Loss: 1.8611953258514404\n",
      "Iteration: 18 \t Train Loss: 1.8005448579788208\n",
      "Iteration: 19 \t Train Loss: 1.6944576501846313\n",
      "Iteration: 20 \t Train Loss: 1.6482499837875366\n",
      "Iteration: 21 \t Train Loss: 1.6814582347869873\n",
      "Iteration: 22 \t Train Loss: 1.5446741580963135\n",
      "Iteration: 23 \t Train Loss: 1.5489412546157837\n",
      "Iteration: 24 \t Train Loss: 1.5032638311386108\n",
      "Iteration: 25 \t Train Loss: 1.5056750774383545\n",
      "Iteration: 26 \t Train Loss: 1.421053409576416\n",
      "Iteration: 27 \t Train Loss: 1.3924611806869507\n",
      "Iteration: 28 \t Train Loss: 1.250032901763916\n",
      "Iteration: 29 \t Train Loss: 1.1266425848007202\n",
      "Iteration: 30 \t Train Loss: 1.2453773021697998\n",
      "Iteration: 31 \t Train Loss: 1.1367247104644775\n",
      "Iteration: 32 \t Train Loss: 0.9906592965126038\n",
      "Iteration: 33 \t Train Loss: 1.059882402420044\n",
      "Iteration: 34 \t Train Loss: 0.9397315382957458\n",
      "Iteration: 35 \t Train Loss: 0.9738790392875671\n",
      "Iteration: 36 \t Train Loss: 0.9149887561798096\n",
      "Iteration: 37 \t Train Loss: 0.844955563545227\n",
      "Iteration: 38 \t Train Loss: 0.7559042572975159\n",
      "Iteration: 39 \t Train Loss: 0.9631808400154114\n",
      "Iteration: 40 \t Train Loss: 0.7342285513877869\n",
      "Iteration: 41 \t Train Loss: 0.6084736585617065\n",
      "Iteration: 42 \t Train Loss: 0.8593817949295044\n",
      "Iteration: 43 \t Train Loss: 0.7748233675956726\n",
      "Iteration: 44 \t Train Loss: 0.823213517665863\n",
      "Iteration: 45 \t Train Loss: 0.7320790886878967\n",
      "Iteration: 46 \t Train Loss: 0.5882406830787659\n",
      "Iteration: 47 \t Train Loss: 0.6588650941848755\n",
      "Iteration: 48 \t Train Loss: 0.739159107208252\n",
      "Iteration: 49 \t Train Loss: 0.5419248342514038\n",
      "Iteration: 50 \t Train Loss: 0.7172450423240662\n",
      "Iteration: 51 \t Train Loss: 0.7495443820953369\n",
      "Iteration: 52 \t Train Loss: 0.6017372012138367\n",
      "Iteration: 53 \t Train Loss: 0.6602138876914978\n",
      "Iteration: 54 \t Train Loss: 0.6085816621780396\n",
      "Iteration: 55 \t Train Loss: 0.5407320261001587\n",
      "Iteration: 56 \t Train Loss: 0.4941432476043701\n",
      "Iteration: 57 \t Train Loss: 0.5209457874298096\n",
      "Iteration: 58 \t Train Loss: 0.6411826014518738\n",
      "Iteration: 59 \t Train Loss: 0.620937168598175\n",
      "Iteration: 60 \t Train Loss: 0.44407451152801514\n",
      "Iteration: 61 \t Train Loss: 0.4677337408065796\n",
      "Iteration: 62 \t Train Loss: 0.6325845122337341\n",
      "Iteration: 63 \t Train Loss: 0.5398642420768738\n",
      "Iteration: 64 \t Train Loss: 0.5865424275398254\n",
      "Iteration: 65 \t Train Loss: 0.5640002489089966\n",
      "Iteration: 66 \t Train Loss: 0.42615005373954773\n",
      "Iteration: 67 \t Train Loss: 0.4782373905181885\n",
      "Iteration: 68 \t Train Loss: 0.436580091714859\n",
      "Iteration: 69 \t Train Loss: 0.5696696639060974\n",
      "Iteration: 70 \t Train Loss: 0.3285200297832489\n",
      "Iteration: 71 \t Train Loss: 0.39308416843414307\n",
      "Iteration: 72 \t Train Loss: 0.39329829812049866\n",
      "Iteration: 73 \t Train Loss: 0.5207264423370361\n",
      "Iteration: 74 \t Train Loss: 0.6086994409561157\n",
      "Iteration: 75 \t Train Loss: 0.47974374890327454\n",
      "Iteration: 76 \t Train Loss: 0.414960652589798\n",
      "Iteration: 77 \t Train Loss: 0.47837594151496887\n",
      "Iteration: 78 \t Train Loss: 0.6105003356933594\n",
      "Iteration: 79 \t Train Loss: 0.4891391694545746\n",
      "Iteration: 80 \t Train Loss: 0.34317469596862793\n",
      "Iteration: 81 \t Train Loss: 0.40135183930397034\n",
      "Iteration: 82 \t Train Loss: 0.6437386870384216\n",
      "Iteration: 83 \t Train Loss: 0.4515256881713867\n",
      "Iteration: 84 \t Train Loss: 0.383162260055542\n",
      "Iteration: 85 \t Train Loss: 0.33330047130584717\n",
      "Iteration: 86 \t Train Loss: 0.365127295255661\n",
      "Iteration: 87 \t Train Loss: 0.41664350032806396\n",
      "Iteration: 88 \t Train Loss: 0.389392226934433\n",
      "Iteration: 89 \t Train Loss: 0.5650549530982971\n",
      "Iteration: 90 \t Train Loss: 0.4849780201911926\n",
      "Iteration: 91 \t Train Loss: 0.4405791759490967\n",
      "Iteration: 92 \t Train Loss: 0.4286767542362213\n",
      "Iteration: 93 \t Train Loss: 0.42859703302383423\n",
      "Iteration: 94 \t Train Loss: 0.4381876587867737\n",
      "Iteration: 95 \t Train Loss: 0.3727293312549591\n",
      "Iteration: 96 \t Train Loss: 0.3233215808868408\n",
      "Iteration: 97 \t Train Loss: 0.3194050192832947\n",
      "Iteration: 98 \t Train Loss: 0.3511112630367279\n",
      "Iteration: 99 \t Train Loss: 0.34982770681381226\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        result = net(data)\n",
    "\n",
    "        loss_val = torch.zeros((1), device=device)\n",
    "        loss_val = loss(result, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        # if counter % 10 == 0:\n",
    "        print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Test Accuracy: 89.44%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        print(f\"Iteration: {counter}\")\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
