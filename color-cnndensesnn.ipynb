{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0,), (1,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "mnist_train = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.CIFAR10(\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(\n",
    "    mnist_train, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    mnist_test, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "\n",
    "\n",
    "class CNNDenseSNN(nn.Module):\n",
    "    def __init__(self, beta=0.9):\n",
    "        super(CNNDenseSNN, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 3 * 3 * 3 * 3)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x0, filters=None):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "\n",
    "        for _ in range(10):\n",
    "            if filters is not None:\n",
    "                outputs = []\n",
    "                for i in range(x0.shape[0]):\n",
    "                    output = F.conv2d(\n",
    "                        x0[i].unsqueeze(0),\n",
    "                        filters[i],\n",
    "                        padding=1,\n",
    "                    )\n",
    "                    outputs.append(output.reshape(3, 32, 32))\n",
    "\n",
    "                x0 = F.relu(x0 + torch.stack(outputs, dim=0) / 24)\n",
    "\n",
    "            x = self.pool(F.relu(self.conv1(x0)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "            x = x.flatten(1)\n",
    "\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            filter_x = F.relu(self.fc3(x))\n",
    "\n",
    "            spk1, mem1 = self.lif1(filter_x, mem1)\n",
    "            filters = spk1.view(-1, 3, 3, 3, 3)\n",
    "\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = CNNDenseSNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 2.3092870712280273\n",
      "Iteration: 1 \t Train Loss: 2.3108346462249756\n",
      "Iteration: 2 \t Train Loss: 2.299056053161621\n",
      "Iteration: 3 \t Train Loss: 2.2966506481170654\n",
      "Iteration: 4 \t Train Loss: 2.320936679840088\n",
      "Iteration: 5 \t Train Loss: 2.291184663772583\n",
      "Iteration: 6 \t Train Loss: 2.2873988151550293\n",
      "Iteration: 7 \t Train Loss: 2.290024518966675\n",
      "Iteration: 8 \t Train Loss: 2.2734811305999756\n",
      "Iteration: 9 \t Train Loss: 2.2841196060180664\n",
      "Iteration: 10 \t Train Loss: 2.2756154537200928\n",
      "Iteration: 11 \t Train Loss: 2.2098610401153564\n",
      "Iteration: 12 \t Train Loss: 2.253403902053833\n",
      "Iteration: 13 \t Train Loss: 2.2462620735168457\n",
      "Iteration: 14 \t Train Loss: 2.1922690868377686\n",
      "Iteration: 15 \t Train Loss: 2.26705265045166\n",
      "Iteration: 16 \t Train Loss: 2.218339681625366\n",
      "Iteration: 17 \t Train Loss: 2.1807172298431396\n",
      "Iteration: 18 \t Train Loss: 2.2445011138916016\n",
      "Iteration: 19 \t Train Loss: 2.165106773376465\n",
      "Iteration: 20 \t Train Loss: 2.2003538608551025\n",
      "Iteration: 21 \t Train Loss: 2.1267523765563965\n",
      "Iteration: 22 \t Train Loss: 2.151615858078003\n",
      "Iteration: 23 \t Train Loss: 2.180826187133789\n",
      "Iteration: 24 \t Train Loss: 2.2016236782073975\n",
      "Iteration: 25 \t Train Loss: 2.117962121963501\n",
      "Iteration: 26 \t Train Loss: 2.1619558334350586\n",
      "Iteration: 27 \t Train Loss: 2.105989933013916\n",
      "Iteration: 28 \t Train Loss: 2.1022801399230957\n",
      "Iteration: 29 \t Train Loss: 2.044890880584717\n",
      "Iteration: 30 \t Train Loss: 2.0806527137756348\n",
      "Iteration: 31 \t Train Loss: 2.1211016178131104\n",
      "Iteration: 32 \t Train Loss: 2.1642379760742188\n",
      "Iteration: 33 \t Train Loss: 2.19657039642334\n",
      "Iteration: 34 \t Train Loss: 2.0467584133148193\n",
      "Iteration: 35 \t Train Loss: 2.056007146835327\n",
      "Iteration: 36 \t Train Loss: 2.243849039077759\n",
      "Iteration: 37 \t Train Loss: 2.1474153995513916\n",
      "Iteration: 38 \t Train Loss: 2.131035327911377\n",
      "Iteration: 39 \t Train Loss: 2.1317179203033447\n",
      "Iteration: 40 \t Train Loss: 2.2147397994995117\n",
      "Iteration: 41 \t Train Loss: 2.1782288551330566\n",
      "Iteration: 42 \t Train Loss: 2.1576311588287354\n",
      "Iteration: 43 \t Train Loss: 2.127887010574341\n",
      "Iteration: 44 \t Train Loss: 2.1639115810394287\n",
      "Iteration: 45 \t Train Loss: 2.1250383853912354\n",
      "Iteration: 46 \t Train Loss: 2.1417152881622314\n",
      "Iteration: 47 \t Train Loss: 2.1449031829833984\n",
      "Iteration: 48 \t Train Loss: 2.078342914581299\n",
      "Iteration: 49 \t Train Loss: 2.0454788208007812\n",
      "Iteration: 50 \t Train Loss: 2.0457050800323486\n",
      "Iteration: 51 \t Train Loss: 2.0192487239837646\n",
      "Iteration: 52 \t Train Loss: 1.982791781425476\n",
      "Iteration: 53 \t Train Loss: 2.159330368041992\n",
      "Iteration: 54 \t Train Loss: 2.060241460800171\n",
      "Iteration: 55 \t Train Loss: 1.9752386808395386\n",
      "Iteration: 56 \t Train Loss: 2.1723861694335938\n",
      "Iteration: 57 \t Train Loss: 2.0401437282562256\n",
      "Iteration: 58 \t Train Loss: 1.997353196144104\n",
      "Iteration: 59 \t Train Loss: 2.2176101207733154\n",
      "Iteration: 60 \t Train Loss: 2.0373597145080566\n",
      "Iteration: 61 \t Train Loss: 2.0101566314697266\n",
      "Iteration: 62 \t Train Loss: 2.051196575164795\n",
      "Iteration: 63 \t Train Loss: 1.9986268281936646\n",
      "Iteration: 64 \t Train Loss: 2.0589699745178223\n",
      "Iteration: 65 \t Train Loss: 2.003777503967285\n",
      "Iteration: 66 \t Train Loss: 1.9799370765686035\n",
      "Iteration: 67 \t Train Loss: 1.9864908456802368\n",
      "Iteration: 68 \t Train Loss: 2.0017004013061523\n",
      "Iteration: 69 \t Train Loss: 2.104973554611206\n",
      "Iteration: 70 \t Train Loss: 1.9433469772338867\n",
      "Iteration: 71 \t Train Loss: 2.0428433418273926\n",
      "Iteration: 72 \t Train Loss: 1.9973267316818237\n",
      "Iteration: 73 \t Train Loss: 1.9575012922286987\n",
      "Iteration: 74 \t Train Loss: 2.128941535949707\n",
      "Iteration: 75 \t Train Loss: 2.0701708793640137\n",
      "Iteration: 76 \t Train Loss: 2.1836721897125244\n",
      "Iteration: 77 \t Train Loss: 2.070347309112549\n",
      "Iteration: 78 \t Train Loss: 2.0559046268463135\n",
      "Iteration: 79 \t Train Loss: 2.027514934539795\n",
      "Iteration: 80 \t Train Loss: 2.130993366241455\n",
      "Iteration: 81 \t Train Loss: 2.0362682342529297\n",
      "Iteration: 82 \t Train Loss: 2.0682904720306396\n",
      "Iteration: 83 \t Train Loss: 2.025949478149414\n",
      "Iteration: 84 \t Train Loss: 2.04599666595459\n",
      "Iteration: 85 \t Train Loss: 1.998794674873352\n",
      "Iteration: 86 \t Train Loss: 2.067563533782959\n",
      "Iteration: 87 \t Train Loss: 2.0340700149536133\n",
      "Iteration: 88 \t Train Loss: 1.9687610864639282\n",
      "Iteration: 89 \t Train Loss: 1.922471523284912\n",
      "Iteration: 90 \t Train Loss: 2.0308187007904053\n",
      "Iteration: 91 \t Train Loss: 2.0476300716400146\n",
      "Iteration: 92 \t Train Loss: 1.8441619873046875\n",
      "Iteration: 93 \t Train Loss: 2.017990827560425\n",
      "Iteration: 94 \t Train Loss: 1.9437114000320435\n",
      "Iteration: 95 \t Train Loss: 1.9325834512710571\n",
      "Iteration: 96 \t Train Loss: 2.0291061401367188\n",
      "Iteration: 97 \t Train Loss: 2.070667266845703\n",
      "Iteration: 98 \t Train Loss: 1.9373596906661987\n",
      "Iteration: 99 \t Train Loss: 2.0659029483795166\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        result = net(data)\n",
    "\n",
    "        loss_val = torch.zeros((1), device=device)\n",
    "        loss_val = loss(result, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        # if counter % 10 == 0:\n",
    "        print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Test Accuracy: 31.32%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        print(f\"Iteration: {counter}\")\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "\n",
    "\n",
    "class CNNDenseSNN(nn.Module):\n",
    "    def __init__(self, beta=0.9):\n",
    "        super(CNNDenseSNN, self).__init__()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 3 * 3 * 3 * 3)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x0, filters=None):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "\n",
    "        for j in range(10):\n",
    "            if filters is not None:\n",
    "                outputs = []\n",
    "                for i in range(x0.shape[0]):\n",
    "                    output = F.conv2d(\n",
    "                        x0[i].unsqueeze(0),\n",
    "                        filters[i],\n",
    "                        padding=1,\n",
    "                    )\n",
    "                    outputs.append(output.reshape(3, 32, 32))\n",
    "\n",
    "                x0 = (\n",
    "                    x0\n",
    "                    + pow(-1, j)\n",
    "                    * torch.exp(torch.tensor((-j / 24)))\n",
    "                    * torch.stack(outputs, dim=0)\n",
    "                    / 24\n",
    "                )\n",
    "\n",
    "            x = self.pool(F.relu(self.conv1(x0)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "            x = x.flatten(1)\n",
    "\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            filter_x = F.relu(self.fc3(x))\n",
    "\n",
    "            spk1, mem1 = self.lif1(filter_x, mem1)\n",
    "            filters = spk1.view(-1, 3, 3, 3, 3)\n",
    "\n",
    "        return self.fc4(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "net = CNNDenseSNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 \t Train Loss: 2.302978038787842\n",
      "Iteration: 1 \t Train Loss: 2.301532506942749\n",
      "Iteration: 2 \t Train Loss: 2.3033957481384277\n",
      "Iteration: 3 \t Train Loss: 2.2999377250671387\n",
      "Iteration: 4 \t Train Loss: 2.291844129562378\n",
      "Iteration: 5 \t Train Loss: 2.293843984603882\n",
      "Iteration: 6 \t Train Loss: 2.289381980895996\n",
      "Iteration: 7 \t Train Loss: 2.273117780685425\n",
      "Iteration: 8 \t Train Loss: 2.2849888801574707\n",
      "Iteration: 9 \t Train Loss: 2.2770159244537354\n",
      "Iteration: 10 \t Train Loss: 2.255248546600342\n",
      "Iteration: 11 \t Train Loss: 2.2611918449401855\n",
      "Iteration: 12 \t Train Loss: 2.263638734817505\n",
      "Iteration: 13 \t Train Loss: 2.2289857864379883\n",
      "Iteration: 14 \t Train Loss: 2.240945339202881\n",
      "Iteration: 15 \t Train Loss: 2.242457866668701\n",
      "Iteration: 16 \t Train Loss: 2.1963462829589844\n",
      "Iteration: 17 \t Train Loss: 2.188034772872925\n",
      "Iteration: 18 \t Train Loss: 2.18349027633667\n",
      "Iteration: 19 \t Train Loss: 2.199744939804077\n",
      "Iteration: 20 \t Train Loss: 2.2231099605560303\n",
      "Iteration: 21 \t Train Loss: 2.0947494506835938\n",
      "Iteration: 22 \t Train Loss: 2.1200389862060547\n",
      "Iteration: 23 \t Train Loss: 2.1562414169311523\n",
      "Iteration: 24 \t Train Loss: 2.0656495094299316\n",
      "Iteration: 25 \t Train Loss: 2.105471134185791\n",
      "Iteration: 26 \t Train Loss: 2.1064534187316895\n",
      "Iteration: 27 \t Train Loss: 2.1585264205932617\n",
      "Iteration: 28 \t Train Loss: 2.1220664978027344\n",
      "Iteration: 29 \t Train Loss: 2.1249022483825684\n",
      "Iteration: 30 \t Train Loss: 2.034660577774048\n",
      "Iteration: 31 \t Train Loss: 1.979310393333435\n",
      "Iteration: 32 \t Train Loss: 2.0367581844329834\n",
      "Iteration: 33 \t Train Loss: 1.9640979766845703\n",
      "Iteration: 34 \t Train Loss: 2.122321844100952\n",
      "Iteration: 35 \t Train Loss: 2.0959320068359375\n",
      "Iteration: 36 \t Train Loss: 2.089545488357544\n",
      "Iteration: 37 \t Train Loss: 2.032104015350342\n",
      "Iteration: 38 \t Train Loss: 2.1445703506469727\n",
      "Iteration: 39 \t Train Loss: 2.124391555786133\n",
      "Iteration: 40 \t Train Loss: 2.038541078567505\n",
      "Iteration: 41 \t Train Loss: 2.0290699005126953\n",
      "Iteration: 42 \t Train Loss: 2.1442835330963135\n",
      "Iteration: 43 \t Train Loss: 1.995193600654602\n",
      "Iteration: 44 \t Train Loss: 2.065037488937378\n",
      "Iteration: 45 \t Train Loss: 2.0933234691619873\n",
      "Iteration: 46 \t Train Loss: 2.0581064224243164\n",
      "Iteration: 47 \t Train Loss: 1.9714614152908325\n",
      "Iteration: 48 \t Train Loss: 1.964613676071167\n",
      "Iteration: 49 \t Train Loss: 1.8636995553970337\n",
      "Iteration: 50 \t Train Loss: 1.886284589767456\n",
      "Iteration: 51 \t Train Loss: 2.004650831222534\n",
      "Iteration: 52 \t Train Loss: 1.9963136911392212\n",
      "Iteration: 53 \t Train Loss: 2.0321195125579834\n",
      "Iteration: 54 \t Train Loss: 1.936262845993042\n",
      "Iteration: 55 \t Train Loss: 2.0150387287139893\n",
      "Iteration: 56 \t Train Loss: 1.95133638381958\n",
      "Iteration: 57 \t Train Loss: 2.0067834854125977\n",
      "Iteration: 58 \t Train Loss: 1.9694303274154663\n",
      "Iteration: 59 \t Train Loss: 1.9251160621643066\n",
      "Iteration: 60 \t Train Loss: 1.8974438905715942\n",
      "Iteration: 61 \t Train Loss: 2.0304629802703857\n",
      "Iteration: 62 \t Train Loss: 1.9639705419540405\n",
      "Iteration: 63 \t Train Loss: 1.844164490699768\n",
      "Iteration: 64 \t Train Loss: 1.8853803873062134\n",
      "Iteration: 65 \t Train Loss: 1.963875651359558\n",
      "Iteration: 66 \t Train Loss: 1.8766101598739624\n",
      "Iteration: 67 \t Train Loss: 2.020171880722046\n",
      "Iteration: 68 \t Train Loss: 1.9047108888626099\n",
      "Iteration: 69 \t Train Loss: 1.8652700185775757\n",
      "Iteration: 70 \t Train Loss: 1.9234422445297241\n",
      "Iteration: 71 \t Train Loss: 1.9669044017791748\n",
      "Iteration: 72 \t Train Loss: 1.9098182916641235\n",
      "Iteration: 73 \t Train Loss: 1.7857111692428589\n",
      "Iteration: 74 \t Train Loss: 1.817001461982727\n",
      "Iteration: 75 \t Train Loss: 1.8126592636108398\n",
      "Iteration: 76 \t Train Loss: 1.9820868968963623\n",
      "Iteration: 77 \t Train Loss: 1.910637378692627\n",
      "Iteration: 78 \t Train Loss: 1.8732478618621826\n",
      "Iteration: 79 \t Train Loss: 1.8075273036956787\n",
      "Iteration: 80 \t Train Loss: 1.988680124282837\n",
      "Iteration: 81 \t Train Loss: 1.7868767976760864\n",
      "Iteration: 82 \t Train Loss: 1.816218614578247\n",
      "Iteration: 83 \t Train Loss: 1.8153235912322998\n",
      "Iteration: 84 \t Train Loss: 1.8375296592712402\n",
      "Iteration: 85 \t Train Loss: 1.838447093963623\n",
      "Iteration: 86 \t Train Loss: 1.750166893005371\n",
      "Iteration: 87 \t Train Loss: 1.7542762756347656\n",
      "Iteration: 88 \t Train Loss: 1.8091541528701782\n",
      "Iteration: 89 \t Train Loss: 1.7915266752243042\n",
      "Iteration: 90 \t Train Loss: 1.886718988418579\n",
      "Iteration: 91 \t Train Loss: 1.871978521347046\n",
      "Iteration: 92 \t Train Loss: 1.80116605758667\n",
      "Iteration: 93 \t Train Loss: 1.7794857025146484\n",
      "Iteration: 94 \t Train Loss: 1.852642297744751\n",
      "Iteration: 95 \t Train Loss: 1.9018456935882568\n",
      "Iteration: 96 \t Train Loss: 1.69362211227417\n",
      "Iteration: 97 \t Train Loss: 1.7828279733657837\n",
      "Iteration: 98 \t Train Loss: 1.691842794418335\n",
      "Iteration: 99 \t Train Loss: 1.8321809768676758\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(1):\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        net.train()\n",
    "        result = net(data)\n",
    "\n",
    "        loss_val = torch.zeros((1), device=device)\n",
    "        loss_val = loss(result, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print train/test loss/accuracy\n",
    "        # if counter % 10 == 0:\n",
    "        print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "        counter += 1\n",
    "\n",
    "        if counter == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "Iteration: 26\n",
      "Iteration: 27\n",
      "Iteration: 28\n",
      "Iteration: 29\n",
      "Iteration: 30\n",
      "Iteration: 31\n",
      "Iteration: 32\n",
      "Iteration: 33\n",
      "Iteration: 34\n",
      "Iteration: 35\n",
      "Iteration: 36\n",
      "Iteration: 37\n",
      "Iteration: 38\n",
      "Iteration: 39\n",
      "Iteration: 40\n",
      "Iteration: 41\n",
      "Iteration: 42\n",
      "Iteration: 43\n",
      "Iteration: 44\n",
      "Iteration: 45\n",
      "Iteration: 46\n",
      "Iteration: 47\n",
      "Iteration: 48\n",
      "Iteration: 49\n",
      "Iteration: 50\n",
      "Iteration: 51\n",
      "Iteration: 52\n",
      "Iteration: 53\n",
      "Iteration: 54\n",
      "Iteration: 55\n",
      "Iteration: 56\n",
      "Iteration: 57\n",
      "Iteration: 58\n",
      "Iteration: 59\n",
      "Iteration: 60\n",
      "Iteration: 61\n",
      "Iteration: 62\n",
      "Iteration: 63\n",
      "Iteration: 64\n",
      "Iteration: 65\n",
      "Iteration: 66\n",
      "Iteration: 67\n",
      "Iteration: 68\n",
      "Iteration: 69\n",
      "Iteration: 70\n",
      "Iteration: 71\n",
      "Iteration: 72\n",
      "Iteration: 73\n",
      "Iteration: 74\n",
      "Iteration: 75\n",
      "Iteration: 76\n",
      "Iteration: 77\n",
      "Test Accuracy: 36.62%\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = net(data)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        print(f\"Iteration: {counter}\")\n",
    "        counter += 1\n",
    "        if counter == 100:\n",
    "            break\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
